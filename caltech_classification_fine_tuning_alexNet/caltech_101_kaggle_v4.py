# -*- coding: utf-8 -*-
"""Caltech_101_
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/caltech-101-kaggle-71b7a1d0-a402-4d1d-ae87-6965ffc7bf2e.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240510/auto/storage/goog4_request%26X-Goog-Date%3D20240510T204603Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D190fe24a44231d3397f7d3786d47ac5e3f779c4d7290f342d46ecf56d16f03fd38cb06ff47ba3a2235f01c99a143ef2d941b40a8678049130ef5c0af2f70840b98f826690be684a0653de33ca0f9f566a0bbfe7fe90df676e004d565e5225aafb7983b9780680ce7a6c7d08a8610562eca54775dc7a8d94b25761e74da448204e10b4286619f26ef99e0542154f25f3464ad78e039812d946f2cd7c7f7a840c66ef5c7ff7f2ffea5aae1b7375cea17009fd5b67c304d2878d7ed1ede9290e2590c4e23181a3407fc73340eab518c4ac8c7b0b8a5de77fa191058c8d3928f16bca08a22df14729a2fae963e7d5b3bd3e94754127ceecedcbfedd67151b29c3155

Install Requirements
"""

# Things left to be done
# tensorboard
# pyqt
#############################################################################
print(" Importing the libraries...")
import wandb
import torch
from torch.utils.data import Dataset, DataLoader, Subset
from torch.utils.data import random_split
from torch.backends import cudnn
from torchvision.models import alexnet
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

torch.manual_seed(1)

"""Set Arguments"""

NUM_CLASSES = 101
DEVICE = 'cuda'
BATCH_SIZE = 32

LR = 1e-3 * 0.9
MOMENTUM = 0.9
WEIGHT_DECAY = 5e-5

NUM_EPOCHS = 10

STEP_SIZE = 20
GAMMA = .1
LOG_FREQUENCY = 10

OPTZ = "adam"
PRETRAINED = True
FINE_TUNE_SETTING = "all" # choose among classifer, all, and features
AUG_PROB = 0.001
AUG_TYPE = "G-AF-C" # can be None

# Set Aragument Parser
print("*" * 50 )
print(" Setting the arg_parser...")
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--epoch', type = int, default = NUM_EPOCHS)
parser.add_argument('--batch_size', type = int, default = BATCH_SIZE)
parser.add_argument('--lr', type = float, default = LR)
parser.add_argument('--momentum', type = float, default = MOMENTUM)
parser.add_argument("--weight_decay",type = float, default = WEIGHT_DECAY)
parser.add_argument("--fine_tune_mode", type = bool, default =  PRETRAINED)
parser.add_argument("--fine_tune_setting", type = str, default = FINE_TUNE_SETTING)
args = parser.parse_args([])
# Do I need to remove the bracket above?
# Note that augmentation type can be added to the possible argparsers!
# You may add different optimizers and models
#args = parser.parse_args(['--param1', '10', '--param2', 'value'])

print("*" * 50 )
print("Initializing the wandb environment...")
wandb.init(
    # set the wandb project where this run will be logged
    project="caltech_101_classification",

    # track hyperparameters and run metadata
    config={

    "learning_rate": args.lr,
    "batch_size": args.batch_size,
    "architecture": "Alexnet",
    "dataset": "Caltech_100",
    "epochs": args.epoch,
    "momentum": args.momentum,
    "weight_decay":args.weight_decay,
    "fine_tune_mode": args.fine_tune_mode,
    "fine_tune_setting": args.fine_tune_setting,
    "augmentation_type": AUG_TYPE,
    "augmentation_probability": AUG_PROB

    }
)

"""Define Data Preprocessing"""

print("*" * 50 )
print("Applying the train and eval transformations...")
from torchvision import transforms
if PRETRAINED == args.fine_tune_mode:
  mean = (.5, .5, .5)
  std = (.229, .224, .225)
else:
  mean = (.485, .456, .406)
  std = (.229, .224, .225)

transform = transforms.Compose([transforms.Resize(256),
                                transforms.CenterCrop(224),
                                transforms.ToTensor(),
                                transforms.Normalize(mean = mean, std = std)])

eval_transform =transforms.Compose([transforms.Resize(256),
                                     transforms.CenterCrop(224),
                                     transforms.ToTensor(),
                                     transforms.Normalize(mean = mean, std =std)])

print("*" * 50 )
print("Applying the augmentation in the training loop and not on the dataset...")
bright_t = transforms.ColorJitter(brightness=[1,2])
contrast_t = transforms.ColorJitter(contrast = [2,5])
saturation_t = transforms.ColorJitter(saturation = [1,3])
hue_t = transforms.ColorJitter(hue = 0.2)
gs_t = transforms.Grayscale(3)
hflip_t = transforms.RandomHorizontalFlip(p = 1)
rp_t = transforms.RandomPerspective(p = 1, distortion_scale = 0.5)
rot_t = transforms.RandomRotation(degrees = 90)

gaussian_t = transforms.GaussianBlur(kernel_size = (3, 3), sigma = (1.0, 1.0))
rAffine_t = transforms.RandomAffine(degrees = 30, translate = (0.1, 0.1), scale = (0.8, 1.2), shear = 10 )

aug_transformations = {
    "CS-HF": transforms.Compose([contrast_t, saturation_t, hflip_t]),
    "H-RP": transforms.Compose([hue_t, rp_t]),
    "B-GS-R": transforms.Compose([bright_t, gs_t, rot_t]),
    "G-AF-C": transforms.Compose([bright_t, gaussian_t, rAffine_t])

    }

if AUG_TYPE is not None:
  aug_transformation = aug_transformations[AUG_TYPE]
  aug_pipeline = transforms.Compose([
      transforms.ToPILImage(),
      transforms.RandomApply([aug_transformation], p = AUG_PROB),
      transforms.ToTensor(),
      transforms.Normalize(mean = mean, std = std)
  ])
else:
  aug_pipeline = transforms.Compose([transforms.Normalize(mean = mean, std = std)])

"""Prepare the Data Class"""

# this way you could run it without adding it to the notebook
# !python caltech_dataset.py

# Following is a test on the pil_loder neglect it unless you want to figure out how it works!
# from PIL import Image
# def pil_loader(path):
#   with open(path, 'rb') as f:
#     img = Image.open(f)
#     return img.convert('RGB')

# split = "train"
# root = "./content/Homework2-Caltech101/101_ObjectCategories/"

# file = open("./content/Homework2-Caltech101/ + split + ".txt", 'r')
# lines = file.readlines()
# path = root + lines[20]
# print(path)
# pil_loader(path.strip())

print("*" * 50 )
print("Initializing the Caltech dataset...")
from torchvision.datasets import VisionDataset
from PIL import Image

import sys
import os.path
import os

def pil_loader(path):
  with open(path, 'rb') as f:
    img = Image.open(f)
    return img.convert('RGB')

class Caltech(VisionDataset):
  def __init__(self, root, split = 'train', transform = None, target_transform = None):
    super(Caltech, self).__init__(root, transform = transform, target_transform=target_transform)
    self.split = split
    self.root = root
    self.transform = transform
    self.image_dict = {}
    self.class_dict = {}
    self.stat_dict = {}

    file = open("/content/Homework2-Caltech101/"+ self.split + ".txt", 'r')
    lines = file.readlines()
    index = 0
    class_index = 0
    for line in lines:
      path = self.root + line
      label = line.split("/")[0]
      if label != "BACKGROUND_Google":
        # if label not in self.stat_dict.keys():
        #   self.stat_dict[label] = 1
        # else:
        #   self.stat_dict[label] += 1
        # if self.stat_dict[label] <= 100:
        if label not in self.class_dict.keys():
          self.class_dict[label] = class_index
          self.image_dict[index] = (pil_loader(path.strip()),class_index)
          class_index += 1
        else:
          target_class = self.class_dict[label]
          self.image_dict[index] = (pil_loader(path.strip()),target_class)

        index += 1


  def __getitem__(self, index):
    image, label = self.image_dict[index]
    if self.transform is not None:
      image = self.transform(image)
    return image, label
  def __len__(self):
    return len(self.image_dict)

print("*" * 50 )
print("Train and test split...")
import pathlib
print(pathlib.Path().resolve())
#if not os.path.isdir('./content/Homework2-Caltech101'):
#  !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git

DATA_DIR = "/content/Homework2-Caltech101/101_ObjectCategories/"

training_data = Caltech(DATA_DIR, 'train', transform = transform)
test_data = Caltech(DATA_DIR, 'test', transform = eval_transform)

#TODO: "the additional background class in not removed"
assert len(training_data.class_dict) == 101, "the additional background class is removed"

# Other ways to split the dataset, you could use train_test split in scikit_learn
# train_indexes = list(range(0, 5000))
# validation_indixes = list(range(5000,6096))

# train_dataset = Subset(training_data, train_indexes)
# val_dataset = Subset(training_data, validation_indixes)

#The other way to seperate the validation dataset is to use random split
train_size =  int(len(training_data) * 0.5)
val_size = len(training_data) - train_size
train_dataset, val_dataset = random_split(training_data, [train_size, val_size])

print("*" * 50 )
print("Gathering statistics on dataset to check class inbalancement...")
len(train_dataset.indices)
class_all_diversity = {}
for i in range(len(training_data.image_dict)):
  _,label = training_data.__getitem__(i)
  if label not in class_all_diversity.keys():
    class_all_diversity[label] = 1
  else:
    class_all_diversity[label] += 1
print(class_all_diversity)

import matplotlib.pyplot as plt
plt.figure(figsize=(35, 6))
plt.bar(range(len(class_all_diversity)), list(class_all_diversity.values()), align='center')
plt.xticks(range(len(class_all_diversity)),list(class_all_diversity.keys()))
plt.show()

# len(train_dataset.dataset.image_dict)
len(train_dataset.indices)
class_val_diversity = {}
for index in val_dataset.indices:
  _,label = training_data.__getitem__(index)
  if label not in class_val_diversity.keys():
    class_val_diversity[label] = 1
  else:
    class_val_diversity[label] += 1
print(class_val_diversity)

import matplotlib.pyplot as plt
plt.bar(range(len(class_val_diversity)), list(class_val_diversity.values()), align='center')
plt.xticks(range(len(class_val_diversity)),list(class_val_diversity.keys()))
plt.show()

# len(train_dataset.dataset.image_dict)
len(train_dataset.indices)
class_diversity = {}
for index in train_dataset.indices:
  _,label = training_data.__getitem__(index)
  if label not in class_diversity.keys():
    class_diversity[label] = 1
  else:
    class_diversity[label] += 1
print(class_diversity)

import matplotlib.pyplot as plt
plt.bar(range(len(class_diversity)), list(class_diversity.values()), align='center')
plt.xticks(range(len(class_diversity)),list(class_diversity.keys()))
plt.show()

# Check dataset size
print("train_dataset size is: {}".format(len(train_dataset)) )
print("val_dataset size is: {}".format(len(val_dataset)))
print("test_dataset size is: {}".format(len(test_data)))

train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, drop_last=True, num_workers=4)
val_loader = DataLoader(val_dataset,batch_size = args.batch_size, shuffle = False, num_workers=4)
test_loader = DataLoader(test_data, batch_size = args.batch_size, shuffle = False, num_workers=4)
print(len(train_loader.dataset))

"""Preparing the Network"""

print("*" * 50 )
print("Defining the model and the summary...")
net = alexnet(pretrained = args.fine_tune_mode)
net.classifier[6] = nn.Linear(4096, NUM_CLASSES)

def print_model_summary(model):
    print(model)
    print("\nModel Summary:")
    total_params = 0
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"Trainable parameter: {name}, Shape: {param.shape}")
            total_params += param.numel()
        else:
            print(f"Frozen parameter: {name}, Shape: {param.shape}")
    print(f"\nTotal Trainable Parameters: {total_params}")

# Usage
print_model_summary(net)

"""Prepare for Training

"""

criterion = nn.CrossEntropyLoss()
if FINE_TUNE_SETTING == args.fine_tune_setting:
  parameters_to_optimize = net.parameters()
elif FINE_TUNE_SETTING == "classifier":
  parameters_to_optimize = net.classifier.parameters()
else:
  parameters_to_optimize = net.features.parameters()


if OPTZ == "sgd":
  optimizer = optim.SGD(parameters_to_optimize, lr = LR , momentum = MOMENTUM, weight_decay= WEIGHT_DECAY )
else:
  optimizer = torch.optim.Adam(parameters_to_optimize, lr=LR, betas=(0.9, 0.999), eps=1e-08, weight_decay=WEIGHT_DECAY )


scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)
print(len(train_loader))

# here we could normalize the images without applying normalization in train_transform
def evaluate(net, data_loader, print_tqdm = False):
  running_corrects = 0
  loss = 0
  net.train(False)
  iterable = tqdm(data_loader) if print_tqdm else data_loader
  for images , labels in iterable:
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    outputs = net(images)
    loss += criterion(outputs, labels).item()
    _,preds = torch.max(outputs.data, 1)
    running_corrects += torch.sum(preds == labels.data).data.item()

  acc = running_corrects/float(len(data_loader.dataset))
  loss = loss/float(len(data_loader))
  return acc, loss

print("*" * 50 )
print("TRAINING LOOP...")
# By default, everything is loaded to cpu
net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda

cudnn.benchmark # Calling this optimizes runtime

current_step = 0
# Start iterating over the epochs
best_test_acc = 0
best_val_acc = 0
for epoch in range(args.epoch):
  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, args.epoch, scheduler.get_last_lr()))

  running_corrects = 0
  # Iterate over the dataset
  for images, labels in train_loader:
    aug_images = []
    for image in images:
      aug_image = aug_pipeline(image)
      aug_images.append(aug_image)
    aug_images = torch.stack(aug_images)
    # Bring data over the device of choice
    aug_images = aug_images.to(DEVICE)
    labels = labels.to(DEVICE)

    net.train() # Sets module in training mode

    # PyTorch, by default, accumulates gradients after each backward pass
    # We need to manually set the gradients to zero before starting a new iteration
    optimizer.zero_grad() # Zero-ing the gradients

    # Forward pass to the network
    outputs = net(aug_images)

    # Compute loss based on output and ground truth
    loss = criterion(outputs, labels)
    # Log loss
    # if current_step % LOG_FREQUENCY == 0:
    #   print('Step {}, Loss {}'.format(current_step, loss.item()))

    # Get predictions
    _, preds = torch.max(outputs.data, 1)

    # Update Corrects
    running_corrects += torch.sum(preds == labels.data).data.item()

    # Compute gradients for each layer and update weights
    loss.backward()  # backward pass: computes gradients
    optimizer.step() # update weights based on accumulated gradients

    current_step += 1

  # Step the scheduler
  scheduler.step()


  # Calculate Accuracy
  val_acc, val_loss = evaluate(net, val_loader, print_tqdm = True)
  test_acc, test_loss = evaluate(net, test_loader, print_tqdm = True)
  accuracy = running_corrects / float(len(train_loader.dataset))
  wandb.log({"epoch":epoch + 1, "train_acc": accuracy * 100, "validation_acc": val_acc * 100, "test_acc": test_acc * 100, "val_loss": val_loss   })
  print("\n In the epoch {}/{} Train accuracy is: {:.2f} % the validation acc: {:.2f} and test acc: {:.2f}, val_loss: {:.2f}".format(epoch + 1 , NUM_EPOCHS, accuracy * 100, val_acc * 100, test_acc * 100, val_loss, test_loss))
  if val_acc >= best_val_acc and test_acc >= best_test_acc:
    checkpoint_path = "checkpoint_epoch_"+ str(epoch +1)+".pt"
    torch.save(net.state_dict(), checkpoint_path)
    artifact = wandb.Artifact("model_checkpoint_" + str(epoch +1), type = "model")
    artifact.add_file(checkpoint_path)
    wandb.log_artifact(artifact)

  # print('\n corrects are: {} and len train_dataset is: {}'.format(running_corrects, len(train_dataset)))
wandb.finish()

# checkpoint = torch.load(checkpoint_path)
# model.load_state_dict(checkpoint)
